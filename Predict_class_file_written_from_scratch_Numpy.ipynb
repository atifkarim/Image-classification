{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this file I am trying to test trained model/ classify image by writing a code only using Numpy. No Keras API has used here. Main part will start by extracting the weight from model file. For loading model.h5 file Keras has used. Preprocessing also has done using some other scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file will work for multi convolution filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from skimage import io, color, exposure, transform\n",
    "from skimage.color import rgb2gray\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split  #it came from update scikit learn. https://stackoverflow.com/questions/40704484/importerror-no-module-named-model-selection\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "NUM_CLASSES = 9 #Used class for the training\n",
    "IMG_SIZE = 48 #required size. This size has also maintained during training. User defined value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract weight from trained model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('/home/atif/image_classification_c++/multi_filter_cpp/traffic_2_filter_no_pad_gray_ep_100_for_cpp.h5')\n",
    "#model = load_model('/home/atif/traffic_model_11_dec_1_filter.h5')\n",
    "layer_list =[]\n",
    "# f = open('/home/atif/path_for_storing_all_layer_info.txt', 'w') #uncomment it if you want to store all layer info at a time.\n",
    "for layer in model.layers:\n",
    "    g=layer.get_config()\n",
    "    h=layer.get_weights()\n",
    "    \n",
    "    layer_list.append(h)\n",
    "#     print (\"g== \",g,\"\\n\") #for printing layer name and verbal info\n",
    "\n",
    "#     print (\"h== \",h,\"\\n\\n\") # for printing layer numeric value, eg: weight, bias value\n",
    "#     print(\"type_of g == \",type(g),\"\\n\")\n",
    "#     print(\"type_of h == \",type(h),\"\\n\")\n",
    "\n",
    "# below lines till f.close() used for writing in text file. To do this you have to uncomment the above line started with f.open() also.\n",
    "\n",
    "#     g1=str(g) # declaring a string variable g1 to store the info of g\n",
    "#     h1=str(h) #declaring a string variable h1 to store the info of h\n",
    "#     g_type=str(type(g)) #declaring a string variable g1 to store the type of g\n",
    "#     h_type=str(type(h)) #declaring a string variable h1 to store the type of h\n",
    "    \n",
    "#     f.write(\"layer_definition: \"+g1+\"\\n\\n\")\n",
    "#     f.write(\"layer_type: \"+g_type+\"\\n\\n\")\n",
    "#     #f.write(\"\\n\")\n",
    "#     f.write(\"layer_weight: \"+h1+\"\\n\\n\")\n",
    "#     f.write(\"weight_type: \"+h_type+\"\\n\\n\\n\")\n",
    "#     f.write(\"\\n\")\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display extracted weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel=layer_list[0][0]\n",
    "conv_kernel=conv_kernel.transpose()\n",
    "#print(\"conv_kernel: \\n\",conv_kernel,\"\\n\\n\")\n",
    "#print(\"conv_kernel shape:\\t\",conv_kernel.shape,\"\\n\\n\")\n",
    "#print(\"conv kernel dimension:\\t\",conv_kernel.ndim,\"\\n\\n\")\n",
    "#print(\"type_conv_kernel:\",type(conv_kernel),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "conv_bias=layer_list[0][1]\n",
    "#print(\"conv_bias_value: \",conv_bias)\n",
    "#print(\"conv_bias ndim: \",conv_bias.ndim,\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "dense_kernel=layer_list[2][0]\n",
    "#print(\"dense_kernel: \\n\",dense_kernel,\"\\n\\n\")\n",
    "#print(\"dense_kernel shape:\\t\",dense_kernel.shape,\"\\n\\n\")\n",
    "#print(\"dense_kernel dimension:\\t\",dense_kernel.ndim,\"\\n\\n\")\n",
    "#print(\"type_dense_kernel:\",type(dense_kernel),\"\\n\")\n",
    "#print(\"dense_kernel size: \",dense_kernel.size,\"\\n\")\n",
    "# dense_1_transpose=dense__1.transpose()\n",
    "# print(\"dense_1_transpose: \",dense_1_transpose,\"\\n\\n\")\n",
    "\n",
    "\n",
    "dense_bias=layer_list[2][1]\n",
    "#print(\"dense_bias: \",dense_bias)\n",
    "#print(\"dense_bias_shape: \",dense_bias.shape)\n",
    "dense_bias=dense_bias.reshape(1,9) # here chenge 5 to the number of your used class\n",
    "#print(\"dense_bias_shape: \",dense_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing convolution kernel weight in a text file to use in CPP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kernel=layer_list[0][0]\n",
    "conv_kernel=conv_kernel.transpose() # This has made to print it like a Matrix form. \n",
    "i_list=[]\n",
    "for i in conv_kernel:\n",
    "#     print(i)\n",
    "    i_list.append(i)\n",
    "#     for k in i:\n",
    "#         print(k)\n",
    "#         i_list.append(k)\n",
    "# print(i_list)\n",
    "i_list_array=[]\n",
    "i_list_array=np.array(i_list)\n",
    "# print(i_list_array.shape)\n",
    "# i_list_array=i_list_array.reshape(2,3,3)\n",
    "# print(i_list_array.ndim)\n",
    "\n",
    "for p in i_list_array:\n",
    "#     for a in p:\n",
    "#         print(a)\n",
    "#    print(p)\n",
    "    ww=str(p)\n",
    "    ww=ww.replace('[','')\n",
    "    ww=ww.replace(']','')\n",
    "    #f=open('/home/atif/conv_k_spy.txt','a') #uncomment from here till f.close() if you want to save text file\n",
    "    #f.write(ww)\n",
    "    #f.write(\"\\n\")\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing convolution kernel bias value in a text file to use in CPP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_bias=layer_list[0][1]\n",
    "conv_bias_list=[]\n",
    "for i in conv_bias:\n",
    "    conv_bias_list.append(i)\n",
    "\n",
    "conv_bias_array=[]\n",
    "conv_bias_array=np.array(conv_bias_list)\n",
    "#print(conv_bias_array)\n",
    "#np.savetxt('/home/atif/conv_b_spy.txt', conv_bias_array, fmt='%1.8e',delimiter=' ')\n",
    "\n",
    "#for x in conv_bias_array:\n",
    "    #print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Dense kernel weight in a text file to use in CPP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_kernel=layer_list[2][0]\n",
    "i_list=[] #declare a list to store the weight of dense kernel\n",
    "for i in dense_kernel:\n",
    "#     print(i)\n",
    "    i_list.append(i) #appended it in the declared list\n",
    "#     for k in i:\n",
    "# #         print(k)\n",
    "#         i_list.append(k)\n",
    "# print(i_list)\n",
    "i_list_array=[] #declared an array\n",
    "i_list_array=np.array(i_list) # store the value of list in the array\n",
    "#print(i_list_array)\n",
    "#np.savetxt('/home/atif/dense_k_spy.txt', i_list_array, fmt='%1.8e',delimiter=' ') #writing on a text file from array\n",
    "\n",
    "# %.8f #you can use it to get float value\n",
    "# fmt='%1.8e' #add this above line after i_list_aray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Dense kernel bias value in a text file to use in CPP code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_bias=layer_list[2][1]\n",
    "dense_bias_list=[]\n",
    "for i in dense_bias:\n",
    "    dense_bias_list.append(i)\n",
    "\n",
    "dense_bias_array=[]\n",
    "dense_bias_array=np.array(dense_bias_list)\n",
    "#print(dense_bias_array)\n",
    "#np.savetxt('/home/atif/dense_b_spy.txt', dense_bias_array, fmt='%1.8e',delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping convolution kernel for further process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"conv_kernel:\\n\",conv_kernel,\"\\n\")\n",
    "#print(\"conv_kernel_shape:\",conv_kernel.shape,\"\\tconv_kernel ndim:\",conv_kernel.ndim,\"\\n\")\n",
    "#print(\"length of conv_kernel:\",len(conv_kernel),\"\\n\")\n",
    "\n",
    "conv_kernel_reshape=conv_kernel.reshape(2,3,3) # 2 for 2 filter. change it according to your filter number\n",
    "#print(\"conv_kernel_reshape:\\n\",conv_kernel_reshape,\"\\n\")\n",
    "#print(\"conv_kernel_reshape shape:\",conv_kernel_reshape.shape,\"\\tconv_kernel_reshape ndim:\",conv_kernel_reshape.ndim,\"\\n\")\n",
    "#print(\"length of conv_kernel_reshape:\",len(conv_kernel_reshape[0]),\"\\n\")\n",
    "\n",
    "convolution_kernel_filter=[]\n",
    "convolution_kernel_filter=np.zeros((2,3,3)) # 2 for 2 filter. change it according to your filter number\n",
    "convolution_kernel_filter[:,:,:]=np.array(conv_kernel_reshape)\n",
    "#print(\"convolution_kernel_filter: \\n\",convolution_kernel_filter,\"\\n\")\n",
    "#print(\"convolution_kernel_filter shape:\",convolution_kernel_filter.shape,\"\\tconvolution_kernel_filter ndim:\",convolution_kernel_filter.ndim,\"\\n\")\n",
    "#print(\"length of convolution_kernel_filter:\",len(convolution_kernel_filter),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Classification\n",
    "## At first processing image, Convolution and apply Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name of loaded image:  /home/atif/image_classification_c++/multi_filter_cpp/test_image/50_speed.ppm\n",
      "Filter  1\n",
      "Filter  2\n",
      "3.466926483846102e-142\n",
      "2.3085899666969084e-48\n",
      "6.995688263318184e-30\n",
      "3.8409075116573936e-15\n",
      "0.9999999999999962\n",
      "class: 4\n",
      "\n",
      "Name of loaded image:  /home/atif/image_classification_c++/multi_filter_cpp/test_image/20_speed.ppm\n",
      "Filter  1\n",
      "Filter "
     ]
    }
   ],
   "source": [
    "def preprocess_img(img):\n",
    "#     uncomment following 5 lines for rgb testing and comment out the rgb2gray line\n",
    "#     Histogram normalization in y\n",
    "#     hsv = color.rgb2hsv(img)\n",
    "#     hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n",
    "#     img = color.hsv2rgb(hsv)\n",
    "\n",
    "    # central scrop\n",
    "    min_side = min(img.shape[:-1])\n",
    "    centre = img.shape[0]//2, img.shape[1]//2\n",
    "    img = img[centre[0]-min_side//2:centre[0]+min_side//2,\n",
    "              centre[1]-min_side//2:centre[1]+min_side//2,\n",
    "              :]\n",
    "    img = rgb2gray(img) #rgb to gray conversion\n",
    "\n",
    "    # rescale to standard size\n",
    "    img = transform.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # roll color axis to axis 0\n",
    "    img = np.rollaxis(img,-1)\n",
    "\n",
    "    return img\n",
    "\n",
    "def conv_(img, conv_filter):\n",
    "#     print(\"\\nconv_ function start to work\\n\")\n",
    "    filter_size = conv_filter.shape[1] #output is 3\n",
    "#     print(\"filter_size: \",filter_size)\n",
    "#     print(\"img shape: \",img.shape)\n",
    "    result = np.zeros((img.shape))\n",
    "    #Looping through the image to apply the convolution operation.\n",
    "    \n",
    "    for x in conv_bias_array:  # to get the value of convolution bias\n",
    "#         print(\"i am the conv_bias: \",x)\n",
    "        \n",
    "        for r in np.uint16(np.arange(filter_size/2.0,img.shape[0]-filter_size/2.0+1)):\n",
    "        \n",
    "            for c in np.uint16(np.arange(filter_size/2.0,img.shape[1]-filter_size/2.0+1)):\n",
    "            \n",
    "                curr_region = img[r-np.uint16(np.floor(filter_size/2.0)):r+np.uint16(np.ceil(filter_size/2.0)), c-np.uint16(np.floor(filter_size/2.0)):c+np.uint16(np.ceil(filter_size/2.0))]\n",
    "        \n",
    "                #Element-wise multipliplication between the current region and the filter.\n",
    "            \n",
    "                curr_result = curr_region * conv_filter\n",
    "#             print(\"curr_result: \",curr_result)\n",
    "                curr_result= curr_result+x\n",
    "                \n",
    "#             print(\"conv_bias: \",conv_bias_new)\n",
    "#                 print(\"new curr res: \",curr_result)\n",
    "                conv_sum = np.sum(curr_result) #Summing the result of multiplication.\n",
    "                result[r, c] = conv_sum#Saving the summation in the convolution layer feature map.\n",
    "#             print(\"conv_sum_shape: \",conv_sum.shape)\n",
    "#         print(\"now x is: \",x)\n",
    "        #print(curr_region)\n",
    "    #Clipping the outliers of the result matrix.\n",
    "    final_result = result[np.uint16(filter_size/2.0):result.shape[0]-np.uint16(filter_size/2.0),np.uint16(filter_size/2.0):result.shape[1]-np.uint16(filter_size/2.0)]\n",
    "#     print(\"\\nconv_ function finish\\n\")\n",
    "    return final_result\n",
    "\n",
    "\n",
    "\n",
    "def conv(img, conv_filter):\n",
    "#     print(\"\\nconv function start to work\")\n",
    "    \n",
    "    if len(img.shape) > 2 or len(conv_filter.shape) > 3: # Check if number of image channels matches the filter depth.\n",
    "        if img.shape[-1] != conv_filter.shape[-1]:\n",
    "            print(\"Error: Number of channels in both image and filter must match.\")\n",
    "            sys.exit()\n",
    "    if conv_filter.shape[1] != conv_filter.shape[2]: # Check if filter dimensions are equal.\n",
    "        print('Error: Filter must be a square matrix. I.e. number of rows and columns must match.')\n",
    "        sys.exit()\n",
    "    if conv_filter.shape[1]%2==0: # Check if filter diemnsions are odd.\n",
    "        print('Error: Filter must have an odd size. I.e. number of rows and columns must be odd.')\n",
    "        sys.exit()\n",
    "\n",
    "    # An empty feature map to hold the output of convolving the filter(s) with the image.\n",
    "    feature_maps = np.zeros((img.shape[0]-conv_filter.shape[1]+1, img.shape[1]-conv_filter.shape[1]+1, conv_filter.shape[0]))\n",
    "\n",
    "    # Convolving the image by the filter(s).\n",
    "    \n",
    "    for filter_num in range(conv_filter.shape[0]):\n",
    "#         print(\"filter num: \",filter_num)\n",
    "        print(\"Filter \", filter_num + 1)\n",
    "        curr_filter = conv_filter[filter_num, :] # getting a filter from the bank.\n",
    "#         print(curr_filter)\n",
    "        #print(\"curr_fiter_shape: \",curr_filter.shape)\n",
    "#         print(\"length of curr_fiter_shape: \",len(curr_filter.shape))\n",
    "\n",
    "\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # Array holding the sum of all feature maps.\n",
    "            for ch_num in range(1, curr_filter.shape[-1]): # Convolving each channel with the image and summing the results.\n",
    "                conv_map = conv_map + conv_(img[:, :, ch_num], \n",
    "                                  curr_filter[:, :, ch_num])\n",
    "        else: # There is just a single channel in the filter.\n",
    "#             print(\"\\nGo to conv_ function \")\n",
    "            conv_map = conv_(img, curr_filter)\n",
    "            \n",
    "        feature_maps[:, :, filter_num] = conv_map # Holding feature map with the current filter.\n",
    "#         print(\"feature_maps from conv_map: \",feature_maps)\n",
    "    return feature_maps # Returning all feature maps.\n",
    "\n",
    "\n",
    "def relu(feature_map):\n",
    "    #Preparing the output of the ReLU activation function.\n",
    "    relu_out = np.zeros(feature_map.shape)\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        for r in np.arange(0,feature_map.shape[0]):\n",
    "            for c in np.arange(0, feature_map.shape[1]):\n",
    "                relu_out[r, c, map_num] = np.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out\n",
    "\n",
    "\n",
    "\n",
    "path = r'/home/atif/image_classification_c++/multi_filter_cpp/test_image/'\n",
    "\n",
    "img_path = glob.glob(path+ '/*.ppm')\n",
    "for image in img_path:\n",
    "    print(\"\\nName of loaded image: \",image)\n",
    "    X_test=[]\n",
    "    X_test.append(preprocess_img(io.imread(image)))\n",
    "    X_test = np.array(X_test)\n",
    "#     plt.imshow(X_test)\n",
    "    X_test = X_test.reshape(IMG_SIZE,IMG_SIZE)\n",
    "    \n",
    "    feature=conv(img=X_test,conv_filter=convolution_kernel_filter) #conv function calling\n",
    "    relu_out=relu(feature) # relu function calling\n",
    "    \n",
    "    # output of feature map / conv function\n",
    "\n",
    "#     print(\"\\nfeature shape: \\n\",feature.shape)\n",
    "\n",
    "    # x_feature_map=np.flipud(feature[0])\n",
    "    transpose_feature_map=feature.transpose()\n",
    "#     print(\"\\ntranspose_feature_map shape: \",transpose_feature_map.shape)\n",
    "#     plt.imshow(transpose_feature_map[0])\n",
    "#     print(\"\\ntranspose_feature_map: \\n\",transpose_feature_map)\n",
    "    \n",
    "#     print(\"\\nrelu_out shape: \",relu_out.shape)\n",
    "    relu_out_transpose=relu_out.transpose()\n",
    "#     print(\"\\nrelu_out_transpose shape: \",relu_out_transpose.shape)\n",
    "#     plt.imshow(relu_out_transpose[0])\n",
    "#     print(\"\\nrelu_out_transpose:\\n\",relu_out_transpose)\n",
    "    \n",
    "# Matrix Multiplication with Dense kernel weight and Convolved Image\n",
    "\n",
    "    flatten_relu_out_transpose=relu_out_transpose.reshape(1,2*46*46)  #if you don't do padd on input image please make it 46*46. how 46 came? \n",
    "                                                                                    #the formula of output size. and 2 for 2 feature map. Here 2 filter so 2 featue map\n",
    "#     print(\"\\nflatten_relu_out_transpose shape: \\n\",flatten_relu_out_transpose.shape)\n",
    "#     print(\"\\ndense_kernel shape: \\n\",dense_kernel.shape,\"\\n\")\n",
    "    matmul_flatt_rel_dense_kernel=np.matmul(flatten_relu_out_transpose,dense_kernel)\n",
    "#     print(\"\\nmatmul_soft_dense_kernel shape: \\n\",matmul_flatt_rel_dense_kernel.shape,\"\\n\")\n",
    "#     print(\"\\nmatmul_soft_dense_kernel: \\n\",matmul_flatt_rel_dense_kernel,\"\\n\")\n",
    "\n",
    "    dense_bias_array=np.array(dense_bias)\n",
    "    dense_bias_array=dense_bias_array.reshape(1,9) # 9 for 9 class\n",
    "#     print(\"\\ndense_bias_array: \\n\",dense_bias_array,\"\\n\")\n",
    "\n",
    "    add_matmul_flatt_rel_dense_kernel_and_dense_bias_array=matmul_flatt_rel_dense_kernel+dense_bias_array\n",
    "#     print(\"\\nvalue add_matmul_flatt_rel_dense_kernel_and_dense2_array: \\n\",add_matmul_flatt_rel_dense_kernel_and_dense_bias_array)\n",
    "    \n",
    "    def softmax_fn(input_array):\n",
    "        e_x=np.exp(input_array-np.max(input_array))\n",
    "        return e_x/e_x.sum(axis=len(e_x.shape)-1)\n",
    "\n",
    "    op= softmax_fn(add_matmul_flatt_rel_dense_kernel_and_dense_bias_array)\n",
    "#     print(\"output of FC layer: \",op,\"\\n\")\n",
    "    \n",
    "#   Show class of loaded image\n",
    "\n",
    "    m=0\n",
    "    k=0\n",
    "    # op=[[0.17095664, 0.24349895, 0.172376,   0.19243606, 0.62073235]]\n",
    "    # op=np.array(op)\n",
    "    # print(op.shape)\n",
    "    # print(type(op))\n",
    "\n",
    "    for h in op:\n",
    "        for index,j in enumerate(h):\n",
    "            o=j\n",
    "            #print(o)\n",
    "            if o>m:\n",
    "                m=o\n",
    "                print(m)\n",
    "                k=index\n",
    "            else:\n",
    "                pass\n",
    "    print('class:',k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
