{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this file I am trying to test model file by writing a code only using Numpy. No Keras API has used here. Main part will start by extracting the weight from model file. For loading model.h5 file Keras has used. Preprocessing also has done using some other library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This file will work for multi convolution filter\n",
    "# Still some issue with convolution bias value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io, color, exposure, transform\n",
    "from skimage.color import rgb2gray\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split  #it came from update scikit learn. https://stackoverflow.com/questions/40704484/importerror-no-module-named-model-selection\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import keras\n",
    "\n",
    "NUM_CLASSES = 9\n",
    "IMG_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /home/atif/image_classification_c++/multi_filter_cpp/test_image/00000_00023.ppm\n",
      "predicted class:  [0]\n",
      "probability:  [[9.9994326e-01 5.4284519e-05 9.5645614e-15 5.9856886e-16 5.2972382e-07\n",
      "  1.9570712e-06 5.6076757e-16 1.8678934e-12 7.6335409e-14]]\n",
      "\n",
      " /home/atif/image_classification_c++/multi_filter_cpp/test_image/stop.ppm\n",
      "predicted class:  [1]\n",
      "probability:  [[5.5017167e-13 9.9084878e-01 1.1804733e-17 5.7442348e-13 2.8099361e-14\n",
      "  7.2337315e-17 9.1510238e-03 1.8840598e-07 2.2821717e-21]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/home/atif/image_classification_c++/multi_filter_cpp/traffic_2_filter_no_pad_gray_ep_100_for_cpp.h5') # path of saved trained model file\n",
    "\n",
    "\n",
    "def preprocess_img(img):\n",
    "#     Histogram normalization in y\n",
    "#     hsv = color.rgb2hsv(img)\n",
    "#     hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n",
    "#     img = color.hsv2rgb(hsv)\n",
    "\n",
    "    # central scrop\n",
    "    min_side = min(img.shape[:-1])\n",
    "    centre = img.shape[0]//2, img.shape[1]//2\n",
    "    img = img[centre[0]-min_side//2:centre[0]+min_side//2,\n",
    "              centre[1]-min_side//2:centre[1]+min_side//2,\n",
    "              :]\n",
    "    img = rgb2gray(img)\n",
    "\n",
    "    # rescale to standard size\n",
    "    img = transform.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # roll color axis to axis 0\n",
    "    img = np.rollaxis(img,-1)\n",
    "\n",
    "    return img\n",
    "\n",
    "import glob\n",
    "\n",
    "path = r'/home/atif/image_classification_c++/multi_filter_cpp/test_image/'\n",
    "\n",
    "img_path = glob.glob(path+ '/*.ppm')  #change ppm to your desired file extension\n",
    "for image in img_path:\n",
    "    X_test=[]\n",
    "    X_test.append(preprocess_img(io.imread(image)))\n",
    "    X_test = np.array(X_test)\n",
    "#     plt.imshow(X_test)\n",
    "    X_test = X_test.reshape(len(X_test),1,IMG_SIZE,IMG_SIZE)\n",
    "    \n",
    "    print(\"\\n\",image)\n",
    "    predicted_class = model.predict_classes(X_test)\n",
    "    print(\"predicted class: \",predicted_class)\n",
    "    \n",
    "    probability = model.predict_proba(X_test)\n",
    "    print(\"probability: \",probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting weight information from  model.h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_kernel: \n",
      " [[[[-1.2551264   1.3612176   2.726472  ]\n",
      "   [-0.81150454  1.0559597   2.4288604 ]\n",
      "   [-2.2760212  -0.57018733 -0.53823084]]]\n",
      "\n",
      "\n",
      " [[[ 1.0510021   0.07008973 -1.1867875 ]\n",
      "   [-0.6361769  -1.4376642  -1.6697451 ]\n",
      "   [-0.12333418 -0.72826564 -0.9773183 ]]]] \n",
      "\n",
      "\n",
      "conv_kernel shape:\t (2, 1, 3, 3) \n",
      "\n",
      "\n",
      "conv kernel dimension:\t 4 \n",
      "\n",
      "\n",
      "type_conv_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      "conv_bias_value:  [0.01443624 1.3674825 ]\n",
      "conv_bias ndim:  1 \n",
      "\n",
      "\n",
      "dense_kernel: \n",
      " [[-0.10735643  0.1640812   0.01809431 ... -0.00946912  0.02848358\n",
      "   0.030178  ]\n",
      " [ 0.01419004  0.04051202 -0.01726801 ...  0.03679919 -0.01469056\n",
      "   0.00603583]\n",
      " [-0.16923815  0.00935     0.05318739 ... -0.03917252  0.00961874\n",
      "   0.03357002]\n",
      " ...\n",
      " [-0.00470943 -0.01924564 -0.03366284 ...  0.00219134 -0.02493748\n",
      "  -0.02235573]\n",
      " [-0.00165659  0.15928343 -0.03851683 ...  0.01218016 -0.01192294\n",
      "  -0.0396612 ]\n",
      " [-0.02387131  0.01669946 -0.0110182  ... -0.04726045 -0.02935163\n",
      "  -0.02188727]] \n",
      "\n",
      "\n",
      "dense_kernel shape:\t (4232, 9) \n",
      "\n",
      "\n",
      "dense_kernel dimension:\t 2 \n",
      "\n",
      "\n",
      "type_dense_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      "dense_kernel size:  38088 \n",
      "\n",
      "dense_bias:  [-0.01253249  0.19295532  0.06694246  0.18706687 -0.24281958  0.13072969\n",
      " -0.02468821 -0.22385639 -0.07379571]\n",
      "dense_bias_shape:  (9,)\n",
      "dense_bias_shape:  (1, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "model = load_model('/home/atif/image_classification_c++/multi_filter_cpp/traffic_2_filter_no_pad_gray_ep_100_for_cpp.h5')\n",
    "\n",
    "layer_list =[]\n",
    "f = open('/home/atif/path_for_storing_all_layer_info.txt', 'w') #uncomment it if you want to store all layer info at a time.\n",
    "for layer in model.layers:\n",
    "    g=layer.get_config()\n",
    "    h=layer.get_weights()\n",
    "    \n",
    "    layer_list.append(h)\n",
    "#     print (\"g== \",g,\"\\n\") #for printing layer name and verbal info\n",
    "\n",
    "#     print (\"h== \",h,\"\\n\\n\") # for printing layer numeric value, eg: weight, bias value\n",
    "#     print(\"type_of g == \",type(g),\"\\n\")\n",
    "#     print(\"type_of h == \",type(h),\"\\n\")\n",
    "\n",
    "# below lines till f.close() used for writing in text file. To do this you have to uncomment the above line started with f.open() also.\n",
    "\n",
    "#     g1=str(g) # declaring a string variable g1 to store the info of g\n",
    "#     h1=str(h) #declaring a string variable h1 to store the info of h\n",
    "#     g_type=str(type(g)) #declaring a string variable g1 to store the type of g\n",
    "#     h_type=str(type(h)) #declaring a string variable h1 to store the type of h\n",
    "    \n",
    "#     f.write(\"layer_definition: \"+g1+\"\\n\\n\")\n",
    "#     f.write(\"layer_type: \"+g_type+\"\\n\\n\")\n",
    "#     #f.write(\"\\n\")\n",
    "#     f.write(\"layer_weight: \"+h1+\"\\n\\n\")\n",
    "#     f.write(\"weight_type: \"+h_type+\"\\n\\n\\n\")\n",
    "#     f.write(\"\\n\")\n",
    "    \n",
    "# f.close()\n",
    "\n",
    "# layer_name=['conv_layer','flatten_layer','dense_layer']\n",
    "\n",
    "\n",
    "\n",
    "#From here the code has started which will extract every layer's info which you can use further in this file\n",
    "        \n",
    "conv_kernel=layer_list[0][0]\n",
    "conv_kernel=conv_kernel.transpose()\n",
    "print(\"conv_kernel: \\n\",conv_kernel,\"\\n\\n\")\n",
    "print(\"conv_kernel shape:\\t\",conv_kernel.shape,\"\\n\\n\")\n",
    "print(\"conv kernel dimension:\\t\",conv_kernel.ndim,\"\\n\\n\")\n",
    "print(\"type_conv_kernel:\",type(conv_kernel),\"\\n\")\n",
    "\n",
    "#conv_kernel_reshape=conv_kernel.reshape(conv_kernel[3],conv_kernel[2],conv_kernel[1],conv_kernel[0])\n",
    "#print(\"re:  \",conv_kernel_reshape.shape)\n",
    "\n",
    "\n",
    "\n",
    "conv_bias=layer_list[0][1]\n",
    "print(\"conv_bias_value: \",conv_bias)\n",
    "print(\"conv_bias ndim: \",conv_bias.ndim,\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "dense_kernel=layer_list[2][0]\n",
    "print(\"dense_kernel: \\n\",dense_kernel,\"\\n\\n\")\n",
    "print(\"dense_kernel shape:\\t\",dense_kernel.shape,\"\\n\\n\")\n",
    "print(\"dense_kernel dimension:\\t\",dense_kernel.ndim,\"\\n\\n\")\n",
    "print(\"type_dense_kernel:\",type(dense_kernel),\"\\n\")\n",
    "print(\"dense_kernel size: \",dense_kernel.size,\"\\n\")\n",
    "# dense_1_transpose=dense__1.transpose()\n",
    "# print(\"dense_1_transpose: \",dense_1_transpose,\"\\n\\n\")\n",
    "\n",
    "\n",
    "dense_bias=layer_list[2][1]\n",
    "print(\"dense_bias: \",dense_bias)\n",
    "print(\"dense_bias_shape: \",dense_bias.shape)\n",
    "dense_bias=dense_bias.reshape(1,9) # here chenge 5 to the number of your used class\n",
    "print(\"dense_bias_shape: \",dense_bias.shape)\n",
    "# print(dense_2[0])\n",
    "\n",
    "\n",
    "# Below code stand for writing all info regarding specific layer in a text file. Not important. Uncomment if you need.\n",
    "\n",
    "# write_conv_kernel=str(conv_kernel)\n",
    "# write_conv_bias=str(conv_bias)\n",
    "# write_dense_kernel=str(dense_kernel)\n",
    "# write_dense_bias=str(dense_bias)\n",
    "\n",
    "\n",
    "# f = open('/home/atif/training_by_several_learning_process/flower_photos/train_model_flower/flw_gray_ch_1_ep_100_no_pad_relu_31_dec_dense_kernel.txt', 'w')\n",
    "\n",
    "# # f.write(\"write_conv_kernel:\\n\\n\"+write_conv_kernel+\"\\n\\n\")\n",
    "\n",
    "# # f.write(\"write_conv_bias:\\n\\n\"+write_conv_bias+\"\\n\\n\")\n",
    "\n",
    "# # f.write(\"write_dense_kernel:\\n\\n\"+write_dense_kernel+\"\\n\\n\")\n",
    "\n",
    "# # f.write(\"write_dense_bias:\\n\\n\"+write_dense_bias+\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing convolution kernel weight in a text file which you can use for C++ purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.2551264   1.3612176   2.726472  ]\n",
      "  [-0.81150454  1.0559597   2.4288604 ]\n",
      "  [-2.2760212  -0.57018733 -0.53823084]]]\n",
      "[[[ 1.0510021   0.07008973 -1.1867875 ]\n",
      "  [-0.6361769  -1.4376642  -1.6697451 ]\n",
      "  [-0.12333418 -0.72826564 -0.9773183 ]]]\n"
     ]
    }
   ],
   "source": [
    "# storing convolution kernel weight\n",
    "\n",
    "conv_kernel=layer_list[0][0]\n",
    "conv_kernel=conv_kernel.transpose() # This has made to print it like a Matrix form. \n",
    "i_list=[]\n",
    "for i in conv_kernel:\n",
    "#     print(i)\n",
    "    i_list.append(i)\n",
    "#     for k in i:\n",
    "#         print(k)\n",
    "#         i_list.append(k)\n",
    "# print(i_list)\n",
    "i_list_array=[]\n",
    "i_list_array=np.array(i_list)\n",
    "# print(i_list_array.shape)\n",
    "# i_list_array=i_list_array.reshape(2,3,3)\n",
    "# print(i_list_array.ndim)\n",
    "\n",
    "for p in i_list_array:\n",
    "#     for a in p:\n",
    "#         print(a)\n",
    "    print(p)\n",
    "    ww=str(p)\n",
    "    ww=ww.replace('[','')\n",
    "    ww=ww.replace(']','')\n",
    "    f=open('/home/atif/conv_k_w.txt','a')\n",
    "    f.write(ww)\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing dense kernel weight directly from the array in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10735643  0.1640812   0.01809431 ... -0.00946912  0.02848358\n",
      "   0.030178  ]\n",
      " [ 0.01419004  0.04051202 -0.01726801 ...  0.03679919 -0.01469056\n",
      "   0.00603583]\n",
      " [-0.16923815  0.00935     0.05318739 ... -0.03917252  0.00961874\n",
      "   0.03357002]\n",
      " ...\n",
      " [-0.00470943 -0.01924564 -0.03366284 ...  0.00219134 -0.02493748\n",
      "  -0.02235573]\n",
      " [-0.00165659  0.15928343 -0.03851683 ...  0.01218016 -0.01192294\n",
      "  -0.0396612 ]\n",
      " [-0.02387131  0.01669946 -0.0110182  ... -0.04726045 -0.02935163\n",
      "  -0.02188727]]\n"
     ]
    }
   ],
   "source": [
    "dense_kernel=layer_list[2][0]\n",
    "i_list=[] #declare a list to store the weight of dense kernel\n",
    "for i in dense_kernel:\n",
    "#     print(i)\n",
    "    i_list.append(i) #appended it in the declared list\n",
    "#     for k in i:\n",
    "# #         print(k)\n",
    "#         i_list.append(k)\n",
    "# print(i_list)\n",
    "i_list_array=[] #declared an array\n",
    "i_list_array=np.array(i_list) # store the value of list in the array\n",
    "print(i_list_array)\n",
    "np.savetxt('/home/atif/dense_k_w.txt', i_list_array, fmt='%1.8e',delimiter=' ') #writing on a text file from array\n",
    "# %.8f\n",
    "# fmt='%1.8e' #add this above line after i_list_aray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing info of convolution bias in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01443624 1.3674825 ]\n"
     ]
    }
   ],
   "source": [
    "# storing conv_bias\n",
    "\n",
    "conv_bias=layer_list[0][1]\n",
    "conv_bias_list=[]\n",
    "for i in conv_bias:\n",
    "    conv_bias_list.append(i)\n",
    "\n",
    "conv_bias_array=[]\n",
    "conv_bias_array=np.array(conv_bias_list)\n",
    "print(conv_bias_array)\n",
    "np.savetxt('/home/atif/conv_k_b.txt', conv_bias_array, fmt='%1.8e',delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing info of dense bias in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01253249  0.19295532  0.06694246  0.18706687 -0.24281958  0.13072969\n",
      " -0.02468821 -0.22385639 -0.07379571]\n"
     ]
    }
   ],
   "source": [
    "#storing dense_bias value\n",
    "\n",
    "dense_bias=layer_list[2][1]\n",
    "dense_bias_list=[]\n",
    "for i in dense_bias:\n",
    "    dense_bias_list.append(i)\n",
    "\n",
    "dense_bias_array=[]\n",
    "dense_bias_array=np.array(dense_bias_list)\n",
    "print(dense_bias_array)\n",
    "np.savetxt('/home/atif/dense_bb.txt', dense_bias_array, fmt='%1.8e',delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below at first, I am making conv_kernel suitable for the approach. Like reshaping and storing in an suitable array to get dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_kernel:\n",
      " [[[[-1.2551264   1.3612176   2.726472  ]\n",
      "   [-0.81150454  1.0559597   2.4288604 ]\n",
      "   [-2.2760212  -0.57018733 -0.53823084]]]\n",
      "\n",
      "\n",
      " [[[ 1.0510021   0.07008973 -1.1867875 ]\n",
      "   [-0.6361769  -1.4376642  -1.6697451 ]\n",
      "   [-0.12333418 -0.72826564 -0.9773183 ]]]] \n",
      "\n",
      "conv_kernel_shape: (2, 1, 3, 3) \tconv_kernel ndim: 4 \n",
      "\n",
      "length of conv_kernel: 2 \n",
      "\n",
      "conv_kernel_reshape:\n",
      " [[[-1.2551264   1.3612176   2.726472  ]\n",
      "  [-0.81150454  1.0559597   2.4288604 ]\n",
      "  [-2.2760212  -0.57018733 -0.53823084]]\n",
      "\n",
      " [[ 1.0510021   0.07008973 -1.1867875 ]\n",
      "  [-0.6361769  -1.4376642  -1.6697451 ]\n",
      "  [-0.12333418 -0.72826564 -0.9773183 ]]] \n",
      "\n",
      "conv_kernel_reshape shape: (2, 3, 3) \tconv_kernel_reshape ndim: 3 \n",
      "\n",
      "length of conv_kernel_reshape: 3 \n",
      "\n",
      "convolution_kernel_filter: \n",
      " [[[-1.25512636  1.36121762  2.7264719 ]\n",
      "  [-0.81150454  1.0559597   2.42886043]\n",
      "  [-2.27602124 -0.57018733 -0.53823084]]\n",
      "\n",
      " [[ 1.05100214  0.07008973 -1.18678749]\n",
      "  [-0.63617688 -1.43766415 -1.66974509]\n",
      "  [-0.12333418 -0.72826564 -0.97731829]]] \n",
      "\n",
      "convolution_kernel_filter shape: (2, 3, 3) \tconvolution_kernel_filter ndim: 3 \n",
      "\n",
      "length of convolution_kernel_filter: 2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"conv_kernel:\\n\",conv_kernel,\"\\n\")\n",
    "print(\"conv_kernel_shape:\",conv_kernel.shape,\"\\tconv_kernel ndim:\",conv_kernel.ndim,\"\\n\")\n",
    "print(\"length of conv_kernel:\",len(conv_kernel),\"\\n\")\n",
    "\n",
    "conv_kernel_reshape=conv_kernel.reshape(2,3,3) # 2 for 2 filter. change it according to your filter number\n",
    "print(\"conv_kernel_reshape:\\n\",conv_kernel_reshape,\"\\n\")\n",
    "print(\"conv_kernel_reshape shape:\",conv_kernel_reshape.shape,\"\\tconv_kernel_reshape ndim:\",conv_kernel_reshape.ndim,\"\\n\")\n",
    "print(\"length of conv_kernel_reshape:\",len(conv_kernel_reshape[0]),\"\\n\")\n",
    "\n",
    "convolution_kernel_filter=[]\n",
    "convolution_kernel_filter=np.zeros((2,3,3)) # 2 for 2 filter. change it according to your filter number\n",
    "convolution_kernel_filter[:,:,:]=np.array(conv_kernel_reshape)\n",
    "print(\"convolution_kernel_filter: \\n\",convolution_kernel_filter,\"\\n\")\n",
    "print(\"convolution_kernel_filter shape:\",convolution_kernel_filter.shape,\"\\tconvolution_kernel_filter ndim:\",convolution_kernel_filter.ndim,\"\\n\")\n",
    "print(\"length of convolution_kernel_filter:\",len(convolution_kernel_filter),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I am reshapingbmy test image for thsi approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_conv shape:  (48, 48) \n",
      "\n",
      "length of X_test_conv:  2 \n",
      "\n",
      "X_test_conv size:  2304 \n",
      "\n",
      "X_test_conv ndim:  [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe9ef8d4da0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_conv = X_test.reshape(IMG_SIZE,IMG_SIZE)\n",
    "print(\"X_test_conv shape: \",X_test_conv.shape,\"\\n\")\n",
    "print(\"length of X_test_conv: \",len(X_test_conv.shape),\"\\n\")\n",
    "print(\"X_test_conv size: \",X_test_conv.size,\"\\n\")\n",
    "print(\"X_test_conv ndim: \",X_test_conv,\"\\n\")\n",
    "plt.imshow(X_test_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In following cell I have added Padding to the input. For my further work to keep simplicity I have not used this but it will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_conv_padded=np.pad(X_test_conv, ((1,1),(1,1)), 'constant')\n",
    "# print(\"X_test_conv_padded size: \",X_test_conv_padded.size,\"\\n\")\n",
    "# plt.imshow(X_test_conv_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for convolution with the convolution filter and then use the created feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01443624 1.3674825 ]\n",
      "Filter  1\n",
      "len:  2\n",
      "CONV function has worked_lala\n",
      "filter_size:  3\n",
      "img shape:  (48, 48)\n",
      "result:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "img:  [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "curr_result:  [[-1.25512636  1.36121762  2.7264719 ]\n",
      " [-0.81150454  1.0559597   2.42886043]\n",
      " [-2.27602124 -0.57018733 -0.53823084]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,3) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3bdb64cadafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# X_test = X_test.reshape(1,IMG_SIZE,IMG_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test_conv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvolution_kernel_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0msoft_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoftmax_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mrelu_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-3bdb64cadafe>\u001b[0m in \u001b[0;36mconv\u001b[0;34m(img, conv_filter)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# There is just a single channel in the filter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CONV function has worked_lala\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mconv_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_map\u001b[0m \u001b[0;31m# Holding feature map with the current filter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_maps\u001b[0m \u001b[0;31m# Returning all feature maps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-3bdb64cadafe>\u001b[0m in \u001b[0;36mconv_\u001b[0;34m(img, conv_filter)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mcurr_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_region\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconv_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"curr_result: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mcurr_result\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcurr_result\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mconv_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m#             print(conv_bias)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#             print(curr_result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,3) (2,) "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "\n",
    "\n",
    "conv_bias=conv_bias\n",
    "print(conv_bias)\n",
    "\n",
    "def conv_(img, conv_filter):\n",
    "    filter_size = conv_filter.shape[1]\n",
    "    print(\"filter_size: \",filter_size)\n",
    "    print(\"img shape: \",img.shape)\n",
    "    result = numpy.zeros((img.shape))\n",
    "    print(\"result: \",result)\n",
    "    #Looping through the image to apply the convolution operation.\n",
    "    for r in numpy.uint16(numpy.arange(filter_size/2.0, \n",
    "                          img.shape[0]-filter_size/2.0+1)):\n",
    "        \n",
    "        for c in numpy.uint16(numpy.arange(filter_size/2.0, \n",
    "                                           img.shape[1]-filter_size/2.0+1)):\n",
    "            \n",
    "            \"\"\"\n",
    "            Getting the current region to get multiplied with the filter.\n",
    "            How to loop through the image and get the region based on \n",
    "            the image and filer sizes is the most tricky part of convolution.\n",
    "            \"\"\"\n",
    "            curr_region = img[r-numpy.uint16(numpy.floor(filter_size/2.0)):r+numpy.uint16(numpy.ceil(filter_size/2.0)), \n",
    "                              c-numpy.uint16(numpy.floor(filter_size/2.0)):c+numpy.uint16(numpy.ceil(filter_size/2.0))]\n",
    "        \n",
    "            #Element-wise multipliplication between the current region and the filter.\n",
    "            print(\"img: \",img)\n",
    "            curr_result = curr_region * conv_filter\n",
    "            print(\"curr_result: \",curr_result)\n",
    "            curr_result= curr_result+conv_bias\n",
    "#             print(conv_bias)\n",
    "#             print(curr_result)\n",
    "            conv_sum = numpy.sum(curr_result) #Summing the result of multiplication.\n",
    "            result[r, c] = conv_sum #Saving the summation in the convolution layer feature map.\n",
    "    #print(curr_region)\n",
    "    #Clipping the outliers of the result matrix.\n",
    "    final_result = result[numpy.uint16(filter_size/2.0):result.shape[0]-numpy.uint16(filter_size/2.0), \n",
    "                          numpy.uint16(filter_size/2.0):result.shape[1]-numpy.uint16(filter_size/2.0)]\n",
    "    return final_result\n",
    "def conv(img, conv_filter):\n",
    "    if len(img.shape) > 2 or len(conv_filter.shape) > 3: # Check if number of image channels matches the filter depth.\n",
    "        if img.shape[-1] != conv_filter.shape[-1]:\n",
    "            print(\"Error: Number of channels in both image and filter must match.\")\n",
    "            sys.exit()\n",
    "    if conv_filter.shape[1] != conv_filter.shape[2]: # Check if filter dimensions are equal.\n",
    "        print('Error: Filter must be a square matrix. I.e. number of rows and columns must match.')\n",
    "        sys.exit()\n",
    "    if conv_filter.shape[1]%2==0: # Check if filter diemnsions are odd.\n",
    "        print('Error: Filter must have an odd size. I.e. number of rows and columns must be odd.')\n",
    "        sys.exit()\n",
    "\n",
    "    # An empty feature map to hold the output of convolving the filter(s) with the image.\n",
    "    feature_maps = numpy.zeros((img.shape[0]-conv_filter.shape[1]+1, \n",
    "                                img.shape[1]-conv_filter.shape[1]+1, \n",
    "                                conv_filter.shape[0]))\n",
    "\n",
    "    # Convolving the image by the filter(s).\n",
    "    for filter_num in range(conv_filter.shape[0]):\n",
    "        print(\"Filter \", filter_num + 1)\n",
    "        curr_filter = conv_filter[filter_num, :] # getting a filter from the bank.\n",
    "        \"\"\" \n",
    "        Checking if there are mutliple channels for the single filter.\n",
    "        If so, then each channel will convolve the image.\n",
    "        The result of all convolutions are summed to return a single feature map.\n",
    "        \"\"\"\n",
    "        print(\"len: \",len(curr_filter.shape))\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            print(\"CONV function has worked\")\n",
    "            conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # Array holding the sum of all feature maps.\n",
    "            for ch_num in range(1, curr_filter.shape[-1]): # Convolving each channel with the image and summing the results.\n",
    "                conv_map = conv_map + conv_(img[:, :, ch_num], \n",
    "                                  curr_filter[:, :, ch_num])\n",
    "        else: # There is just a single channel in the filter.\n",
    "            print(\"CONV function has worked_lala\")\n",
    "            conv_map = conv_(img, curr_filter)\n",
    "        feature_maps[:, :, filter_num] = conv_map # Holding feature map with the current filter.\n",
    "    return feature_maps # Returning all feature maps.\n",
    "\n",
    "\n",
    "def relu(feature_map):\n",
    "    #Preparing the output of the ReLU activation function.\n",
    "    relu_out = numpy.zeros(feature_map.shape)\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        for r in numpy.arange(0,feature_map.shape[0]):\n",
    "            for c in numpy.arange(0, feature_map.shape[1]):\n",
    "                relu_out[r, c, map_num] = numpy.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out\n",
    "\n",
    "def softmax_fn(input_array):\n",
    "    e_x=np.exp(input_array-np.max(input_array))\n",
    "    return e_x/e_x.sum(axis=len(e_x.shape)-1)\n",
    "\n",
    "\n",
    "# X_test = X_test.reshape(1,IMG_SIZE,IMG_SIZE)\n",
    "\n",
    "feature=conv(img=X_test_conv,conv_filter=convolution_kernel_filter)\n",
    "soft_max=softmax_fn(feature)\n",
    "relu_out=relu(feature)\n",
    "# # print(img.shape)\n",
    "# print(feature[:,:,0])\n",
    "# print(\"\\n\\n\",feature[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## following cell is showing the output of 'conv' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 46, 2)\n",
      "transpose_feature_map shape:  (2, 46, 46)\n",
      "transpose_feature_map: \n",
      " [[[12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  ...\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]]\n",
      "\n",
      " [[ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  ...\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(feature.shape)\n",
    "\n",
    "# x_feature_map=np.flipud(feature[0])\n",
    "transpose_feature_map=feature.transpose()\n",
    "print(\"transpose_feature_map shape: \",transpose_feature_map.shape)\n",
    "plt.imshow(transpose_feature_map[0])\n",
    "print(\"transpose_feature_map: \\n\",transpose_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## following cell works with relu. if you want to apply relu on feature map then execute it. for this you have to also change in the called model in the training phase and train again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu_out shape:  (46, 46, 2)\n",
      "relu_out_transpose shape:  (2, 46, 46)\n",
      "relu_out_transpose:\n",
      " [[[12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  ...\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]\n",
      "  [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "   12.92143934]]\n",
      "\n",
      " [[ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  ...\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]\n",
      "  [ 5.16180015  5.16180015  5.16180015 ...  5.16180015  5.16180015\n",
      "    5.16180015]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"relu_out shape: \",relu_out.shape)\n",
    "relu_out_transpose=relu_out.transpose()\n",
    "print(\"relu_out_transpose shape: \",relu_out_transpose.shape)\n",
    "plt.imshow(relu_out_transpose[0])\n",
    "print(\"relu_out_transpose:\\n\",relu_out_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following cell I have done matrix_multiplication between relu output and dense_kernel. Finally added dense_bias value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_relu_out_transpose shape:  (1, 4232)\n",
      "dense_kernel shape:  (4232, 9) \n",
      "\n",
      "matmul_soft_dense_kernel shape (1, 9) \n",
      "\n",
      "matmul_soft_dense_kernel:  [[-185.0576935   -25.80008745   29.31363173   99.0706058    47.40671623\n",
      "    15.71534522  -20.6711238   -17.72083701  -15.07523071]] \n",
      "\n",
      "dense_bias_array:  [[-0.01253249  0.19295532  0.06694246  0.18706687 -0.24281958  0.13072969\n",
      "  -0.02468821 -0.22385639 -0.07379571]] \n",
      "\n",
      "value add_matmul_flatt_rel_dense_kernel_and_dense2_array:\n",
      " [[-185.07022599  -25.60713214   29.38057419   99.25767267   47.16389665\n",
      "    15.84607491  -20.695812    -17.9446934   -15.14902641]]\n"
     ]
    }
   ],
   "source": [
    "flatten_relu_out_transpose=relu_out_transpose.reshape(1,2*46*46)  #if you don't do padd on input image please make it 46*46. how 46 came? \n",
    "                                                                                    #the formula of output size.\n",
    "print(\"flatten_relu_out_transpose shape: \",flatten_relu_out_transpose.shape)\n",
    "\n",
    "print(\"dense_kernel shape: \",dense_kernel.shape,\"\\n\")\n",
    "\n",
    "matmul_flatt_rel_dense_kernel=np.matmul(flatten_relu_out_transpose,dense_kernel)\n",
    "print(\"matmul_soft_dense_kernel shape\",matmul_flatt_rel_dense_kernel.shape,\"\\n\")\n",
    "print(\"matmul_soft_dense_kernel: \",matmul_flatt_rel_dense_kernel,\"\\n\")\n",
    "\n",
    "dense_bias_array=np.array(dense_bias)\n",
    "dense_bias_array=dense_bias_array.reshape(1,9)\n",
    "print(\"dense_bias_array: \",dense_bias_array,\"\\n\")\n",
    "\n",
    "add_matmul_flatt_rel_dense_kernel_and_dense_bias_array=matmul_flatt_rel_dense_kernel+dense_bias_array\n",
    "print(\"value add_matmul_flatt_rel_dense_kernel_and_dense2_array:\\n\",add_matmul_flatt_rel_dense_kernel_and_dense_bias_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here, I have applied softmax to the output and then calculated the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of FC layer:  [[3.29581298e-124 5.91431279e-055 4.49533146e-031 1.00000000e+000\n",
      "  2.37662430e-023 5.95392508e-037 8.03273652e-053 1.25793775e-051\n",
      "  2.05969017e-050]] \n",
      "\n",
      "3.295812978283471e-124\n",
      "5.9143127862059375e-55\n",
      "4.495331455858062e-31\n",
      "1.0\n",
      "class: 3\n"
     ]
    }
   ],
   "source": [
    "def softmax_fn(input_array):\n",
    "    e_x=np.exp(input_array-np.max(input_array))\n",
    "    return e_x/e_x.sum(axis=len(e_x.shape)-1)\n",
    "\n",
    "op= softmax_fn(add_matmul_flatt_rel_dense_kernel_and_dense_bias_array)\n",
    "print(\"output of FC layer: \",op,\"\\n\")\n",
    "########################\n",
    "# Folowing code for finding class##\n",
    "########################\n",
    "m=0\n",
    "k=0\n",
    "# op=[[0.17095664, 0.24349895, 0.172376,   0.19243606, 0.62073235]]\n",
    "# op=np.array(op)\n",
    "# print(op.shape)\n",
    "# print(type(op))\n",
    "\n",
    "for h in op:\n",
    "    \n",
    "    for index,j in enumerate(h):\n",
    "    \n",
    "        o=j\n",
    "        #print(o)\n",
    "        if o>m:\n",
    "            m=o\n",
    "            print(m)\n",
    "            k=index\n",
    "        else:\n",
    "            pass\n",
    "print('class:',k)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use it for multi filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter  1\n",
      "curr_fiter_shape:  (3, 3)\n",
      "go to conv_ function \n",
      "\n",
      "conv_ function start to work\n",
      "\n",
      "filter_size:  3\n",
      "img shape:  (48, 48)\n",
      "\n",
      "conv_ function finish\n",
      "\n",
      "[[12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "  12.92143934]\n",
      " [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "  12.92143934]\n",
      " [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "  12.92143934]\n",
      " ...\n",
      " [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "  12.92143934]\n",
      " [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "  12.92143934]\n",
      " [12.92143934 12.92143934 12.92143934 ... 12.92143934 12.92143934\n",
      "  12.92143934]]\n",
      "Filter  2\n",
      "curr_fiter_shape:  (3, 3)\n",
      "go to conv_ function \n",
      "\n",
      "conv_ function start to work\n",
      "\n",
      "filter_size:  3\n",
      "img shape:  (48, 48)\n",
      "\n",
      "conv_ function finish\n",
      "\n",
      "[[5.16180015 5.16180015 5.16180015 ... 5.16180015 5.16180015 5.16180015]\n",
      " [5.16180015 5.16180015 5.16180015 ... 5.16180015 5.16180015 5.16180015]\n",
      " [5.16180015 5.16180015 5.16180015 ... 5.16180015 5.16180015 5.16180015]\n",
      " ...\n",
      " [5.16180015 5.16180015 5.16180015 ... 5.16180015 5.16180015 5.16180015]\n",
      " [5.16180015 5.16180015 5.16180015 ... 5.16180015 5.16180015 5.16180015]\n",
      " [5.16180015 5.16180015 5.16180015 ... 5.16180015 5.16180015 5.16180015]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "# conv_bias=conv_bias\n",
    "# conv_bias_new=1.3\n",
    "# print(conv_bias_new)\n",
    "\n",
    "def conv_(img, conv_filter):\n",
    "    print(\"\\nconv_ function start to work\\n\")\n",
    "    filter_size = conv_filter.shape[1] #output is 3\n",
    "    print(\"filter_size: \",filter_size)\n",
    "    print(\"img shape: \",img.shape)\n",
    "    result = numpy.zeros((img.shape))\n",
    "#     result= result.reshape(1,result)\n",
    "#     print(\"result: \",result)\n",
    "    #Looping through the image to apply the convolution operation.\n",
    "#     for x,i in enumerate(conv_bias):\n",
    "        \n",
    "    for r in numpy.uint16(numpy.arange(filter_size/2.0,img.shape[0]-filter_size/2.0+1)):\n",
    "        \n",
    "        for c in numpy.uint16(numpy.arange(filter_size/2.0,img.shape[1]-filter_size/2.0+1)):\n",
    "            \n",
    "            curr_region = img[r-numpy.uint16(numpy.floor(filter_size/2.0)):r+numpy.uint16(numpy.ceil(filter_size/2.0)), c-numpy.uint16(numpy.floor(filter_size/2.0)):c+numpy.uint16(numpy.ceil(filter_size/2.0))]\n",
    "        \n",
    "            #Element-wise multipliplication between the current region and the filter.\n",
    "            \n",
    "            curr_result = curr_region * conv_filter\n",
    "#             print(\"curr_result: \",curr_result)\n",
    "            curr_result= curr_result+1.2\n",
    "#             print(\"conv_bias: \",conv_bias_new)\n",
    "#             print(\"new curr res: \",curr_result)\n",
    "            conv_sum = numpy.sum(curr_result) #Summing the result of multiplication.\n",
    "            result[r, c] = conv_sum#Saving the summation in the convolution layer feature map.\n",
    "#             print(\"conv_sum_shape: \",conv_sum.shape)\n",
    "    #print(curr_region)\n",
    "    #Clipping the outliers of the result matrix.\n",
    "    final_result = result[numpy.uint16(filter_size/2.0):result.shape[0]-numpy.uint16(filter_size/2.0),numpy.uint16(filter_size/2.0):result.shape[1]-numpy.uint16(filter_size/2.0)]\n",
    "    print(\"\\nconv_ function finish\\n\")\n",
    "    return final_result\n",
    "\n",
    "\n",
    "\n",
    "def conv(img, conv_filter):\n",
    "#     print(\"conv_filter:: \", conv_filter.shape[0])\n",
    "    \n",
    "    if len(img.shape) > 2 or len(conv_filter.shape) > 3: # Check if number of image channels matches the filter depth.\n",
    "        if img.shape[-1] != conv_filter.shape[-1]:\n",
    "            print(\"Error: Number of channels in both image and filter must match.\")\n",
    "            sys.exit()\n",
    "    if conv_filter.shape[1] != conv_filter.shape[2]: # Check if filter dimensions are equal.\n",
    "        print('Error: Filter must be a square matrix. I.e. number of rows and columns must match.')\n",
    "        sys.exit()\n",
    "    if conv_filter.shape[1]%2==0: # Check if filter diemnsions are odd.\n",
    "        print('Error: Filter must have an odd size. I.e. number of rows and columns must be odd.')\n",
    "        sys.exit()\n",
    "\n",
    "    # An empty feature map to hold the output of convolving the filter(s) with the image.\n",
    "    feature_maps = numpy.zeros((img.shape[0]-conv_filter.shape[1]+1, img.shape[1]-conv_filter.shape[1]+1, conv_filter.shape[0]))\n",
    "\n",
    "    # Convolving the image by the filter(s).\n",
    "    \n",
    "    for filter_num in range(conv_filter.shape[0]):\n",
    "#         print(\"filter num: \",filter_num)\n",
    "        print(\"Filter \", filter_num + 1)\n",
    "        curr_filter = conv_filter[filter_num, :] # getting a filter from the bank.\n",
    "#         print(curr_filter)\n",
    "        print(\"curr_fiter_shape: \",curr_filter.shape)\n",
    "#         print(\"length of curr_fiter_shape: \",len(curr_filter.shape))\n",
    "\n",
    "\n",
    "        if len(curr_filter.shape) > 2:\n",
    "            print(\"CONV function has worked\")\n",
    "            conv_map = conv_(img[:, :, 0], curr_filter[:, :, 0]) # Array holding the sum of all feature maps.\n",
    "            for ch_num in range(1, curr_filter.shape[-1]): # Convolving each channel with the image and summing the results.\n",
    "                conv_map = conv_map + conv_(img[:, :, ch_num], \n",
    "                                  curr_filter[:, :, ch_num])\n",
    "        else: # There is just a single channel in the filter.\n",
    "            print(\"go to conv_ function \")\n",
    "            conv_map = conv_(img, curr_filter)\n",
    "            print(conv_map)\n",
    "            \n",
    "        feature_maps[:, :, filter_num] = conv_map # Holding feature map with the current filter.\n",
    "#         print(\"feature_maps from conv_map: \",feature_maps)\n",
    "    return feature_maps # Returning all feature maps.\n",
    "\n",
    "\n",
    "def relu(feature_map):\n",
    "    #Preparing the output of the ReLU activation function.\n",
    "    relu_out = numpy.zeros(feature_map.shape)\n",
    "    for map_num in range(feature_map.shape[-1]):\n",
    "        for r in numpy.arange(0,feature_map.shape[0]):\n",
    "            for c in numpy.arange(0, feature_map.shape[1]):\n",
    "                relu_out[r, c, map_num] = numpy.max([feature_map[r, c, map_num], 0])\n",
    "    return relu_out\n",
    "\n",
    "def softmax_fn(input_array):\n",
    "    e_x=np.exp(input_array-np.max(input_array))\n",
    "    return e_x/e_x.sum(axis=len(e_x.shape)-1)\n",
    "\n",
    "\n",
    "# X_test = X_test.reshape(1,IMG_SIZE,IMG_SIZE)\n",
    "\n",
    "feature=conv(img=X_test_conv,conv_filter=convolution_kernel_filter)\n",
    "relu_out=relu(feature)\n",
    "# soft_max=softmax_fn(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
