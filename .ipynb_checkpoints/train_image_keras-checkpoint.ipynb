{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage import io, color, exposure, transform\n",
    "from skimage.color import rgb2gray\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split  #it came from update scikit learn. https://stackoverflow.com/questions/40704484/importerror-no-module-named-model-selection\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D,Conv1D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import keras\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "IMG_SIZE = 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for preprocessing image. Able for RGB & Gray Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for gray scale\n",
    "def preprocess_img(img):\n",
    "    # Histogram normalization in y\n",
    "#     hsv = color.rgb2hsv(img)\n",
    "#     hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n",
    "#     img = color.hsv2rgb(hsv)\n",
    "\n",
    "    # central scrop\n",
    "    min_side = min(img.shape[:-1])\n",
    "    centre = img.shape[0]//2, img.shape[1]//2\n",
    "    img = img[centre[0]-min_side//2:centre[0]+min_side//2,\n",
    "              centre[1]-min_side//2:centre[1]+min_side//2,\n",
    "              :]\n",
    "    img = rgb2gray(img)\n",
    "\n",
    "    # rescale to standard size\n",
    "    img = transform.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # roll color axis to axis 0\n",
    "    img = np.rollaxis(img,-1)\n",
    "\n",
    "    return img\n",
    "\n",
    "def get_class(img_path):\n",
    "    return int(img_path.split('/')[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the image and label in array. Run this cell if you need to do training. For testing no need to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atif/anaconda3/envs/venv/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/atif/anaconda3/envs/venv/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/3673\n",
      "Processed 2000/3673\n",
      "Processed 3000/3673\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "labels = []\n",
    "root_dir = '/home/atif/training_by_several_learning_process/flower_photos/flower_train_images/'\n",
    "#path='/home/atif/training_by_several_learning_process/flower_photos/00000/'\n",
    "\n",
    "#all_img_paths = glob.glob(path+ '5547758_eea9edfd54_n_000.jpg')\n",
    "\n",
    "all_img_paths = glob.glob(os.path.join(root_dir, '*/*.jpg')) #I have done the training with .ppm format image. If another type of image will come \n",
    "                                                                                    #them .ppm will be changed by that extension\n",
    "np.random.shuffle(all_img_paths)\n",
    "for img_path in all_img_paths:\n",
    "    try:\n",
    "        img = preprocess_img(io.imread(img_path))\n",
    "        label = get_class(img_path)\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "        if len(imgs)%1000 == 0: print(\"Processed {}/{}\".format(len(imgs), len(all_img_paths)))\n",
    "            #print(\"get it 2\")\n",
    "    except (IOError, OSError):\n",
    "        print('missed', img_path)\n",
    "        pass\n",
    "\n",
    "X = np.array(imgs, dtype='float32') #Keeping the image as an array\n",
    "Y = np.eye(NUM_CLASSES, dtype='uint8')[labels] #labels of the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3673, 48, 48)\n",
      "(3673, 1, 48, 48)\n",
      "4\n",
      "(1, 48, 48)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array(imgs, dtype='float32')\n",
    "print(X.shape)\n",
    "# plt.imshow(X[0])\n",
    "plt.imshow(X[0],cmap=\"gray\")\n",
    "X = X.reshape(len(imgs),1,IMG_SIZE,IMG_SIZE) # write (IMG_SIZE,IMG_SIZE,1 if you want channel last; 1= grayscale;3=RGB)\n",
    "# plt.imshow(X[0],cmap=\"gray\")\n",
    "print(X.shape)\n",
    "print(X.ndim)\n",
    "print(X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3673, 1, 48, 48)\n",
      "(3673, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making a model. Run it for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is for understanding the inner calculation of CNN that's why I have started witha minimal layer as well as model.\n",
    "# Increase the filter number and layer if you want a good result\n",
    "#Conv2D(1, (3, 3) >> here 1 = number of filter. (3,3) = kernel height and width\n",
    "\n",
    "def cnn_model():\n",
    "#      padding='same'\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(1, (3, 3),\n",
    "                     input_shape=(1,IMG_SIZE, IMG_SIZE),\n",
    "                     activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = cnn_model()\n",
    "\n",
    "lr = 0.01\n",
    "sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=sgd,\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 1, 46, 46)         10        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2116)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 10585     \n",
      "=================================================================\n",
      "Total params: 10,595\n",
      "Trainable params: 10,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase. Storing the model also for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2938 samples, validate on 735 samples\n",
      "Epoch 1/5\n",
      "2938/2938 [==============================] - 1s 476us/step - loss: 1.6103 - acc: 0.2332 - val_loss: 1.5972 - val_acc: 0.2490\n",
      "Epoch 2/5\n",
      "2938/2938 [==============================] - 1s 299us/step - loss: 1.6020 - acc: 0.2434 - val_loss: 1.5979 - val_acc: 0.2490\n",
      "Epoch 3/5\n",
      "2938/2938 [==============================] - 1s 288us/step - loss: 1.6019 - acc: 0.2434 - val_loss: 1.5968 - val_acc: 0.2490\n",
      "Epoch 4/5\n",
      "2938/2938 [==============================] - 1s 307us/step - loss: 1.6020 - acc: 0.2434 - val_loss: 1.5964 - val_acc: 0.2490\n",
      "Epoch 5/5\n",
      "2938/2938 [==============================] - 1s 289us/step - loss: 1.6019 - acc: 0.2434 - val_loss: 1.5977 - val_acc: 0.2490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3cf8a34dd8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return lr * (0.1 ** int(epoch / 10))\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "model.fit(X, Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,\n",
    "          #np.resize(img, (-1, <image shape>)\n",
    "          callbacks=[LearningRateScheduler(lr_schedule),ModelCheckpoint('/home/atif/training_by_several_learning_process/number_classify/rgb_2_gray/Image-classification/gray_flower_ep_5_ch_1.h5', save_best_only=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /home/atif/training_by_several_learning_process/flower_photos/flower_test_image/20407896403_a50fef58ac_n.jpg\n",
      "predicted class:  [1]\n",
      "probability:  [[0.17356831 0.24834245 0.17232047 0.18868636 0.21708232]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atif/anaconda3/envs/venv/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/atif/anaconda3/envs/venv/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/home/atif/training_by_several_learning_process/number_classify/rgb_2_gray/Image-classification/gray_flower_ep_5_ch_1.h5')\n",
    "#for gray scale\n",
    "def preprocess_img(img):\n",
    "#     Histogram normalization in y\n",
    "#     hsv = color.rgb2hsv(img)\n",
    "#     hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n",
    "#     img = color.hsv2rgb(hsv)\n",
    "\n",
    "    # central scrop\n",
    "    min_side = min(img.shape[:-1])\n",
    "    centre = img.shape[0]//2, img.shape[1]//2\n",
    "    img = img[centre[0]-min_side//2:centre[0]+min_side//2,\n",
    "              centre[1]-min_side//2:centre[1]+min_side//2,\n",
    "              :]\n",
    "    img = rgb2gray(img)\n",
    "\n",
    "    # rescale to standard size\n",
    "    img = transform.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # roll color axis to axis 0\n",
    "    img = np.rollaxis(img,-1)\n",
    "\n",
    "    return img\n",
    "\n",
    "import glob\n",
    "\n",
    "path = r'/home/atif/training_by_several_learning_process/flower_photos/flower_test_image/'\n",
    "\n",
    "img_path = glob.glob(path+ '/*.jpg')\n",
    "for image in img_path:\n",
    "    X_test=[]\n",
    "    X_test.append(preprocess_img(io.imread(image)))\n",
    "    X_test = np.array(X_test)\n",
    "#     plt.imshow(X_test)\n",
    "    X_test = X_test.reshape(len(X_test),1,IMG_SIZE,IMG_SIZE)\n",
    "    \n",
    "    print(\"\\n\",image)\n",
    "    predicted_class = model.predict_classes(X_test)\n",
    "    print(\"predicted class: \",predicted_class)\n",
    "    \n",
    "    probability = model.predict_proba(X_test)\n",
    "    print(\"probability: \",probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 48, 48)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_test.ndim)\n",
    "# plt.imshow(X_test[0])\n",
    "X_test = X_test.reshape(len(X_test),1,IMG_SIZE,IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting the weight from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g==  {'name': 'conv2d_1', 'trainable': True, 'batch_input_shape': (None, 1, 48, 48), 'dtype': 'float32', 'filters': 1, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_first', 'dilation_rate': (1, 1), 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} \n",
      "\n",
      "h==  [array([[[[-0.03033464]],\n",
      "\n",
      "        [[ 0.23801574]],\n",
      "\n",
      "        [[-0.56720567]]],\n",
      "\n",
      "\n",
      "       [[[ 0.24555111]],\n",
      "\n",
      "        [[-0.6949715 ]],\n",
      "\n",
      "        [[-0.06389805]]],\n",
      "\n",
      "\n",
      "       [[[-0.5152609 ]],\n",
      "\n",
      "        [[ 0.05478185]],\n",
      "\n",
      "        [[ 0.33108518]]]], dtype=float32), array([-0.35551634], dtype=float32)] \n",
      "\n",
      "\n",
      "g==  {'name': 'flatten_1', 'trainable': True, 'data_format': 'channels_first'} \n",
      "\n",
      "h==  [] \n",
      "\n",
      "\n",
      "g==  {'name': 'dense_1', 'trainable': True, 'units': 5, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} \n",
      "\n",
      "h==  [array([[ 0.02572025,  0.03265782, -0.01833312,  0.02983849,  0.05503121],\n",
      "       [-0.05123737,  0.01785515,  0.00602751, -0.05107643, -0.00307727],\n",
      "       [-0.00622287, -0.00842408,  0.01207713, -0.04138358,  0.05845851],\n",
      "       ...,\n",
      "       [ 0.03343552, -0.00784191,  0.05150667, -0.00721849, -0.04127815],\n",
      "       [ 0.01287838,  0.00819396,  0.03280327, -0.0312414 ,  0.00180113],\n",
      "       [ 0.03425178, -0.04119075, -0.01142644,  0.00084342,  0.01159499]],\n",
      "      dtype=float32), array([-0.13164842,  0.22658895, -0.13886377, -0.04813359,  0.09205692],\n",
      "      dtype=float32)] \n",
      "\n",
      "\n",
      "conv_kernel: \n",
      " [[[[-0.03033464  0.24555111 -0.5152609 ]\n",
      "   [ 0.23801574 -0.6949715   0.05478185]\n",
      "   [-0.56720567 -0.06389805  0.33108518]]]] \n",
      "\n",
      "\n",
      "conv_kernel shape:\t (1, 1, 3, 3) \n",
      "\n",
      "\n",
      "conv kernel dimension:\t 4 \n",
      "\n",
      "\n",
      "type_conv_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      "conv_bias_value:  [-0.35551634]\n",
      "conv_bias ndim:  1 \n",
      "\n",
      "\n",
      "dense_kernel: \n",
      " [[ 0.02572025  0.03265782 -0.01833312  0.02983849  0.05503121]\n",
      " [-0.05123737  0.01785515  0.00602751 -0.05107643 -0.00307727]\n",
      " [-0.00622287 -0.00842408  0.01207713 -0.04138358  0.05845851]\n",
      " ...\n",
      " [ 0.03343552 -0.00784191  0.05150667 -0.00721849 -0.04127815]\n",
      " [ 0.01287838  0.00819396  0.03280327 -0.0312414   0.00180113]\n",
      " [ 0.03425178 -0.04119075 -0.01142644  0.00084342  0.01159499]] \n",
      "\n",
      "\n",
      "dense_kernel shape:\t (2116, 5) \n",
      "\n",
      "\n",
      "dense_kernel dimension:\t 2 \n",
      "\n",
      "\n",
      "type_dense_kernel: <class 'numpy.ndarray'> \n",
      "\n",
      "dense_kernel size:  10580 \n",
      "\n",
      "dense_bias:  [-0.13164842  0.22658895 -0.13886377 -0.04813359  0.09205692]\n",
      "dense_bias_shape:  (5,)\n",
      "dense_bias_shape:  (1, 5)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "model = load_model('/home/atif/training_by_several_learning_process/number_classify/rgb_2_gray/Image-classification/gray_flower_ep_5_ch_1.h5')\n",
    "x_list=[]\n",
    "layer_list =[]\n",
    "i=0\n",
    "for layer in model.layers:\n",
    "    g=layer.get_config()\n",
    "    h=layer.get_weights()\n",
    "    \n",
    "    layer_list.append(h)\n",
    "    i=i+1\n",
    "    print (\"g== \",g,\"\\n\")\n",
    "\n",
    "    print (\"h== \",h,\"\\n\\n\")\n",
    "    #print(\"type_of g == \",type(g),\"\\n\")\n",
    "    #print(\"type_of h == \",type(h),\"\\n\")\n",
    "layer_name=['conv_layer','flatten_layer','dense_layer']\n",
    "        \n",
    "conv_kernel=layer_list[0][0]\n",
    "conv_kernel=conv_kernel.transpose()\n",
    "print(\"conv_kernel: \\n\",conv_kernel,\"\\n\\n\")\n",
    "print(\"conv_kernel shape:\\t\",conv_kernel.shape,\"\\n\\n\")\n",
    "print(\"conv kernel dimension:\\t\",conv_kernel.ndim,\"\\n\\n\")\n",
    "print(\"type_conv_kernel:\",type(conv_kernel),\"\\n\")\n",
    "#conv_kernel_reshape=conv_kernel.reshape(conv_kernel[3],conv_kernel[2],conv_kernel[1],conv_kernel[0])\n",
    "#print(\"re:  \",conv_kernel_reshape.shape)\n",
    "\n",
    "conv_bias=layer_list[0][1]\n",
    "print(\"conv_bias_value: \",conv_bias)\n",
    "print(\"conv_bias ndim: \",conv_bias.ndim,\"\\n\\n\")\n",
    "\n",
    "dense_kernel=layer_list[2][0]\n",
    "# conv_kernel=conv_kernel.transpose()\n",
    "print(\"dense_kernel: \\n\",dense_kernel,\"\\n\\n\")\n",
    "print(\"dense_kernel shape:\\t\",dense_kernel.shape,\"\\n\\n\")\n",
    "print(\"dense_kernel dimension:\\t\",dense_kernel.ndim,\"\\n\\n\")\n",
    "print(\"type_dense_kernel:\",type(dense_kernel),\"\\n\")\n",
    "print(\"dense_kernel size: \",dense_kernel.size,\"\\n\")\n",
    "# dense_1_transpose=dense__1.transpose()\n",
    "# print(\"dense_1_transpose: \",dense_1_transpose,\"\\n\\n\")\n",
    "\n",
    "\n",
    "dense_bias=layer_list[2][1]\n",
    "print(\"dense_bias: \",dense_bias)\n",
    "print(\"dense_bias_shape: \",dense_bias.shape)\n",
    "dense_bias=dense_bias.reshape(1,5)\n",
    "print(\"dense_bias_shape: \",dense_bias.shape)\n",
    "# print(dense_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
